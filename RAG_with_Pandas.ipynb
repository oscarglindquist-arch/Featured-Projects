{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "kCA4dloztz4I",
   "metadata": {
    "id": "kCA4dloztz4I"
   },
   "source": [
    "## RAG for Log Analysis using Pandas Operations\n",
    "In this notebook the approach that we have taken is to build a query pipeline and have our LLM take in our queries in natural langauge, transform them into executable python code using methods and functions available in the pandas library, the code is run using a Pandas Instructions Parser (ready built by LlamaIndex) and then the LLM interprets the results of the code and formulates a response. This notebook is built based on [this code](https://docs.llamaindex.ai/en/stable/examples/pipeline/query_pipeline_pandas/) by LlamaIndex. Because we use the ready built Pandas Instructions Parser, the sort of queries the RAG can answer are somewhat limited to only operations performed with the Pandas library.\n",
    "\n",
    "A possible option to overcome this limitation in the future is using the Pandas Instruction Parser module as a template to build an Instructions Parser that allows importing any library and can run code from any library. In an attempt towards this direction, the local source code of the Pandas Instructions Parser was edited and an optional keyword argument to allow the imports of other libraries was added. But it also involved disabling a lot of the safety checks LlamaIndex had put into place so it was precarious at best. The edited code can be found in the tentative_instructions_parser directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70d702-1782-42f0-b7cd-a2696cf75cb8",
   "metadata": {
    "id": "8a70d702-1782-42f0-b7cd-a2696cf75cb8"
   },
   "outputs": [],
   "source": [
    "# Only needed once\n",
    "!pip install -q llama-index==0.10.36 llama-index-embeddings-ollama==0.1.2 \\\n",
    "             llama-index-embeddings-openai==0.1.9 llama-index-llms-ollama==0.1.3 \\\n",
    "             llama-index-llms-openai==0.1.19 llama-index-vector-stores-postgres==0.1.7 \\\n",
    "             llama-index-vector-stores-qdrant==0.2.8 ollama==0.1.9 qdrant-client==1.9.1  \\\n",
    "             llama-index-experimental pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082c03b-cf44-45cf-a05d-aee3e50bd291",
   "metadata": {
    "id": "4082c03b-cf44-45cf-a05d-aee3e50bd291"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_pipeline import (\n",
    "    QueryPipeline as QP,\n",
    "    Link,\n",
    "    InputComponent,\n",
    ")\n",
    "from llama_index.experimental.query_engine.pandas import (\n",
    "    PandasInstructionParser,\n",
    ")\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import PromptTemplate, get_response_synthesizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import logging\n",
    "from llama_index.experimental.query_engine import PandasQueryEngine\n",
    "\n",
    "\n",
    "from dataportal import DataportalClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ZTUjLoo4JKu",
   "metadata": {
    "id": "6ZTUjLoo4JKu"
   },
   "source": [
    "### Loading in Data\n",
    "The section that follows involves loading in the data. The next two cells are for loading in data from the WARA-Ops portal, while the third cell after this involves loading in some data that we have stored on disk. Please load in your data through whichever method you prefer, the only requirement is that it should be in a dataframe called \"df\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68533c31-6ee1-46da-9151-0a106d488a55",
   "metadata": {
    "id": "68533c31-6ee1-46da-9151-0a106d488a55",
    "outputId": "0eed5517-4024-4148-d331-f361afd5ef80"
   },
   "outputs": [],
   "source": [
    "# loading the data from WARA-Ops\n",
    "dataset = 'ERDClogs-parsed' # Enter the name of a dataset you have access to\n",
    "token = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VyIjp7InVzZXJuYW1lIjoiZXJjeDQ3OCIsImxvZ2dlZEluIjp0cnVlLCJncm91cHMiOlsiY249d2FyYW9wcy1vcmctdW5pdmVyc2l0eSxjbj1ncm91cHMsY249YWNjb3VudHMsZGM9eGVyY2VzLGRjPWxhbiIsImNuPXdhcmFvcHMtdXNlcixjbj1ncm91cHMsY249YWNjb3VudHMsZGM9eGVyY2VzLGRjPWxhbiJdLCJvcmdhbml6YXRpb24iOiJ1bml2ZXJzaXR5In0sInV1aWQiOiI5NDk0NWQzOC1iYmYzLTQzNTMtYWEwYi04ZTFhMDI0NzM3ZDgiLCJpYXQiOjE3MTkyMTU2ODMsImV4cCI6MTczNTAyNjg4M30.z5Pc5CTpS5rVFfZ6nRTQu2xIJqKmljmWesix8AATepQ' # Enter your token from the dataportal here\n",
    "\n",
    "client = DataportalClient(token)\n",
    "fileList = client.fromDataset(dataset).listFiles(returnList=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7553ec-8431-4be8-a4b6-53202fa7c360",
   "metadata": {
    "id": "9c7553ec-8431-4be8-a4b6-53202fa7c360"
   },
   "outputs": [],
   "source": [
    "df = client.getData(fileList[1]['FileID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e69f99-2247-4006-9ef0-bc349701b93d",
   "metadata": {
    "id": "62e69f99-2247-4006-9ef0-bc349701b93d"
   },
   "outputs": [],
   "source": [
    "# Loading in data from disk (this particular file contains the nova logs for an entire day as extracted with the data_cleaning notebook which is just an extended version of the\n",
    "# function in the cell below that performs data extraction.) The 'data' directory also contains some other nova logs for ease of use to avoid having to load in or extract data\n",
    "# from the WARA-Ops portal all the time for a few experiments\n",
    "df = pd.read_pickle('./nova_days/nova_day_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Jh5t-B265Ui",
   "metadata": {
    "id": "-Jh5t-B265Ui"
   },
   "source": [
    "This is an optional step, in case you want to work with only a specific value from a column so you can filter out only those values. Please be careful of the spelling and type in the exact column names and values as it won't work if you misspell them. We have the cell displaying your data head to help with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cq9GZLrk8OEL",
   "metadata": {
    "id": "cq9GZLrk8OEL"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ef0c4-75e8-467b-80ee-f55e644e8a58",
   "metadata": {
    "id": "251ef0c4-75e8-467b-80ee-f55e644e8a58"
   },
   "outputs": [],
   "source": [
    "# function that does data cleaning, you'll tell llm you want to work with so, so, so column with these values\n",
    "def data_extraction(df, column:str, value:str):\n",
    "    \"\"\"function finds all instances of a value in a certain column and returns a dataframe with only the rows value appears in\"\"\"\n",
    "    clean_df = df[df[column] == value]\n",
    "    #df[df['request_id'] == 'ff1f86e1-4855-403b-ac40-5cf1b3d165cb']\n",
    "    return clean_df\n",
    "\n",
    "# pl\n",
    "value = input(\"Which value do you want to trace?\")\n",
    "column = input(\"What column does it belong in?\")\n",
    "\n",
    "try:\n",
    "    df = data_extraction(df, column, value)\n",
    "except:\n",
    "    print(f'Could not find {column}. Check your spelling and try again')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qs8WXhDJ8cZs",
   "metadata": {
    "id": "Qs8WXhDJ8cZs"
   },
   "source": [
    "### Defining the elements in the pipeline\n",
    "The elements in the pipeline are the string prompts (instruction_str, pandas_prompt_str) that guide the LLM and provide a CoT (Chain of Thought) for it to follow in translating the query from natural language, to executable python code. The pandas_output_parser then processes the coe generated by the LLM and finally the response_synthesis_prompt_str takes in the original input query, the output of the pandas_code, the expression that was derived and passes it all into an LLM to formulate a natural language response. A DAG showcases the visual represent of this query pipeline to provide a more intuitive understanding of how it works. More information about query pipelines can be found [here.](https://docs.llamaindex.ai/en/stable/module_guides/querying/pipeline/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d4f6f-12c0-48bc-bfad-bad56a792330",
   "metadata": {
    "id": "414d4f6f-12c0-48bc-bfad-bad56a792330"
   },
   "outputs": [],
   "source": [
    "# This cell defines the elements in the query pipeline\n",
    "\n",
    "instruction_str = (\n",
    "    \"1. Convert the query to executable Python code using Pandas.\\n\"\n",
    "    \"2. The final line of code should be a Python expression that can be called with the `eval()` function.\\n\"\n",
    "    \"3. The code should represent a solution to the query.\\n\"\n",
    "    \"4. PRINT ONLY THE EXPRESSION. \\n\"\n",
    "    \"5. Do not quote the expression.\\n\"\n",
    ")\n",
    "\n",
    "pandas_prompt_str = (\n",
    "    \"You are working with a pandas dataframe in Python.\\n\"\n",
    "    \"The name of the dataframe is `df`.\\n\"\n",
    "    \"This is the result of `print(df.head())`:\\n\"\n",
    "    \"{df_str}\\n\\n\"\n",
    "    \"Follow these instructions:\\n\"\n",
    "    \"{instruction_str}\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Expression: \\n\"\n",
    ")\n",
    "response_synthesis_prompt_str = (\n",
    "    \"Given an input question, synthesize a response from the query results.\\n\"\n",
    "    \"Query: {query_str}\\n\\n\"\n",
    "    \"Pandas Instructions (optional):\\n{pandas_instructions}\\n\\n\"\n",
    "    \"Pandas Output: {pandas_output}\\n\\n\"\n",
    "    \"Response: \"\n",
    ")\n",
    "\n",
    "pandas_prompt = PromptTemplate(pandas_prompt_str).partial_format(\n",
    "    instruction_str=instruction_str, df_str=df.head(5)\n",
    ")\n",
    "pandas_output_parser = PandasInstructionParser(df)\n",
    "response_synthesis_prompt = PromptTemplate(response_synthesis_prompt_str)\n",
    "llm = Ollama(model=\"llama3:70b\", base_url='http://10.129.20.4:9090', request_timeout=360)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f529d-003a-406a-b18c-34f118de0c25",
   "metadata": {
    "id": "2a4f529d-003a-406a-b18c-34f118de0c25"
   },
   "outputs": [],
   "source": [
    "# the elements defined in the cell above are instantiated as modules and the connections between the elements are made using the chain and link syntax. The chain is for\n",
    "# sequential connections while the links are for the elements that have more than linear connections, (have several things plugged into it, the response_synthesis_prompt)\n",
    "qp = QP(\n",
    "    modules={\n",
    "        \"input\": InputComponent(),\n",
    "        \"pandas_prompt\": pandas_prompt,\n",
    "        \"llm1\": llm,\n",
    "        \"pandas_output_parser\": pandas_output_parser,\n",
    "        \"response_synthesis_prompt\": response_synthesis_prompt,\n",
    "        \"llm2\": llm,\n",
    "    },\n",
    "    verbose=True,\n",
    ")\n",
    "qp.add_chain([\"input\", \"pandas_prompt\", \"llm1\", \"pandas_output_parser\"])\n",
    "qp.add_links(\n",
    "    [\n",
    "        Link(\"input\", \"response_synthesis_prompt\", dest_key=\"query_str\"),\n",
    "        Link(\n",
    "            \"llm1\", \"response_synthesis_prompt\", dest_key=\"pandas_instructions\"\n",
    "        ),\n",
    "        Link(\n",
    "            \"pandas_output_parser\",\n",
    "            \"response_synthesis_prompt\",\n",
    "            dest_key=\"pandas_output\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# add link from response synthesis prompt to llm2\n",
    "qp.add_link(\"response_synthesis_prompt\", \"llm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cce8ec-6fee-42b0-8129-6dd879b17b53",
   "metadata": {
    "id": "73cce8ec-6fee-42b0-8129-6dd879b17b53",
    "outputId": "6ef6690a-14ee-4b4c-e81f-3b49262757a8"
   },
   "outputs": [],
   "source": [
    "# visualization of the query pipeline\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(qp.dag)\n",
    "\n",
    "# Save the network as \"text2sql_dag.html\"\n",
    "net.write_html(\"pandas_dag.html\")\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Read the contents of the HTML file\n",
    "with open(\"pandas_dag.html\", \"r\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Display the HTML content\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iaooleU8EU92",
   "metadata": {
    "id": "iaooleU8EU92"
   },
   "source": [
    "### Running queries\n",
    "Typically a query pipeline is run with \"qp.run\" and this gives a query engine functionality where you ask single questions over your data. It doesn't have a \"as_chat_engine\" functionality built-in. So, to enable us to have a chatbot of sorts with a back and forth allowing follow-up questions to your queries, the cell below is run in a while loop so that it takes you into a REPL. However, something to keep in mind is that the LLM loses track of the conversation and loses memory of your conversation after a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf36b3-ee43-4b04-b569-aacd8ea3ba00",
   "metadata": {
    "id": "0aaf36b3-ee43-4b04-b569-aacd8ea3ba00",
    "outputId": "0efb8f8e-254e-46f7-cb31-0dbe17bafe55"
   },
   "outputs": [],
   "source": [
    "query = \"\"\n",
    "while query != \"Stop\":\n",
    "    print()\n",
    "    query = input(\"To stop LLM write Stop, insert your question HERE: \")\n",
    "    if query != \"Stop\":\n",
    "        response = qp.run(query_str=query)\n",
    "        print()\n",
    "        print(f\"\\033[1m{response.message.content}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4558969f-546d-46e9-8e40-a536278c71dc",
   "metadata": {
    "id": "4558969f-546d-46e9-8e40-a536278c71dc",
    "outputId": "bd78d779-dbf1-43b6-c3aa-e791303b6a9a"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
