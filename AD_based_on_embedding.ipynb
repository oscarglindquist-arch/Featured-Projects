{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058b8455-c723-4e2b-b2f0-ef0890d332b8",
   "metadata": {},
   "source": [
    "# IForest anomaly detection on logs\n",
    "\n",
    "A method to let the LLM only analyze the most anomalous logs based on an anomaly detection algorithm. The anomaly detection is on the embedded logs on a 1024 dimensional room.\n",
    "\n",
    "This notebook is based on an [article](https://qdrant.tech/articles/detecting-coffee-anomalies/) about anomaly detection in a qdrant vectorstore. We will try to implement an anomaly detection on the embedded vector, to find the most abnormal logs. The end goal is to be able to store the vectors in the qdrant and use this as both a classic RAG example and at the same time be able to execute the anomaly detection on the vectorstore. However qdrant has a restriction, where you can only implement their specific anomaly detection algorithm and therefore we will not store the embeddings in the vectorstore in this example. Instead we will keep the vectors as a large dataframe during our anomaly detection.\n",
    "\n",
    "All logs gets an anomaly score, which makes it easy to set a threshold and evaluate all logs above the threshold. However one major problem with this method is it takes a long time to run. It is analyzing the most abnormal logs that you want or decide seems like an anomaly. The amount of logs analyzed can be alot, based on the effectiveness of the anomaly detection algorithm. It also embedds all logs, which takes a long time to do with the current embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e91781-d2df-487b-87fd-ccc4dfab9828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Old dependencies that works ###\n",
    "!pip install -q pyod==2.0.1 tensorflow==2.17.0 pythresh==0.3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49a4a5-e15b-4195-8a03-6bc9f4df388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.models.iforest import IForest\n",
    "from pythresh.thresholds.zscore import ZSCORE\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9bee4a-37dc-4e2a-9f74-19b5ed23ccd8",
   "metadata": {},
   "source": [
    "First step is to read your file of logs or get it from the portal. To get all logs from the portal watch the [data cleaning notebook](data_cleaning.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdccd15-73d2-4853-9ba6-e0baa37e5ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in your pickle file with logs\n",
    "logs = pd.read_pickle('data/nova_small_eval_set')\n",
    "\n",
    "#Adds logs as a list\n",
    "log_list = logs.values.tolist()\n",
    "log_list_strings = [' '.join(map(str, sublist)) for sublist in log_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d910b1-7ff8-4092-8447-6806fa2f9286",
   "metadata": {},
   "source": [
    "In this step we will embedd all logs, this will take a long time if you have a large set of logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda743a2-e7e3-4d8c-bef2-7e2b6a7cf22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Settings\n",
    "# choose your preferred llm by setting the model variable = the model you want set by default to llama3:8b\n",
    "\n",
    "model70b='llama3:70b'\n",
    "model8b='llama3:8b'\n",
    "modelphi3mini='phi3:mini'\n",
    "modelphi3medium='phi3:medium'\n",
    "model3p1_8 = \"llama3.1:8b\" \n",
    "model3p1_70 = \"llama3.1:70b\"\n",
    "modelgemma9 = \"gemma2:9b\"\n",
    "modelgemma27 = \"gemma2:27b\"\n",
    "modelqwen7=\"qwen2:7b\"\n",
    "modelqwen = \"qwen2\"\n",
    "\n",
    "# set your preference of model and collection name here üëáüèæ\n",
    "model = model3p1_70\n",
    "\n",
    "#Creates your embedding and LLM model\n",
    "embed_model = OllamaEmbedding(model_name='mxbai-embed-large', base_url='http://10.129.20.4:9090')\n",
    "llm = Ollama(model=model, base_url='http://10.129.20.4:9090', request_timeout=360)\n",
    "\n",
    "start = time.time()\n",
    "#Embedding\n",
    "embed_list = embed_model._get_text_embeddings(log_list_strings)\n",
    "df_embed = pd.DataFrame(embed_list)\n",
    "print(f\"Embedding time {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386246c-3e2b-4203-9988-699bd4313d3f",
   "metadata": {},
   "source": [
    "Now we will setup our anomaly detection method. In our example we will use [IForest](https://pyod.readthedocs.io/en/latest/_modules/pyod/models/iforest.html) from pyod. However this can easily be replaced with another algorithm if preferred. We will also plot the result of the anomaly detection with a score of color. Keep in mind, this plot may not represent the clusters well, because it only plots 2 of 1024 dimensions. We also extract the log with highest anomaly-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da2fed-b491-4674-8e33-dc88483955bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Anomaly detection algorithm\n",
    "clf_name = 'IForest'\n",
    "clf = IForest(contamination = 0.001, random_state=123) #contamination should probably be as low as possible\n",
    "clf.fit(df_embed)\n",
    "print(\"Original threshold:\", clf.threshold_)\n",
    "print(\"max score:\", max(clf.decision_scores_))\n",
    "\n",
    "thres = ZSCORE()\n",
    "y_scores = clf.decision_scores_  # raw outlier scores (higher score = more likely to be anomaly)\n",
    "# binary labels (0: inliers, 1: outliers)\n",
    "y_pred = thres.eval(y_scores)\n",
    "\n",
    "# Plot the result of the first 2 dimension (total of 1024 dimension, so it isn't a good representation visually)\n",
    "figures, ax1 = plt.subplots(1,1)\n",
    "ax1.scatter(df_embed.iloc[:,0], df_embed.iloc[:,1], c = y_scores)\n",
    "ax1.set_title('Found outliers by score')\n",
    "\n",
    "#Find the \"largest\" anomaly (highest score)\n",
    "value_to_find = max(y_scores)\n",
    "\n",
    "# Use numpy.where() to find the position\n",
    "position = np.where(y_scores == value_to_find)\n",
    "\n",
    "# Print the position\n",
    "print(\"Position of the value:\", position[0])\n",
    "\n",
    "#most abnormal log\n",
    "anom = logs.iloc[position[0],:] #It should be request id '18ea01d4-10c8-4280-9419-41e8e0b2550d'\n",
    "anom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf01f4b-04b6-42cd-b615-33359d77f2ef",
   "metadata": {},
   "source": [
    "Now we will use the LLM with a premade prompt to analyze the single log based on instructions from the prompt. This does not compare or analyze based on other logs in your system, but rather the pretrained information about logs from the LLM. The prompt can be fine-tuned to better analyze the log that you can see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e064bac8-6c9b-4281-a6d7-9fa600ee0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple setup of an LLM explaining the log with highest score without context to other logs\n",
    "\n",
    "# May be changed for your specific usercase\n",
    "prompt =  f\"\"\"You are a log anlysis expert that will analyse a specifc log. Based on this log: {anom.to_string()}. \n",
    "Can you interpret the log and explain why it could be some potential problems with it.\n",
    "This logs differ from the other logs in a dataset, but I would like you to explain what it actually says in a more extensive analysis.\n",
    "You don't need to explain all variables that are normal. Instead only bring up the parts that seems to be potential issues.\n",
    "Keep the answer short and clear explanations\"\"\"\n",
    "\n",
    "response = llm.complete(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90204feb-10c2-481b-a18c-a8e80c8dcdaf",
   "metadata": {},
   "source": [
    "### Extra information\n",
    "#### Timeseries\n",
    "This is a start to multiple cases. One interesting future project would be to use this part with a more preprocessed data. Then it would find more interesting anomalies within eg one UUID, which could give a more relevant results. It could also be added into different types of timeseries data, which the IForest find abnormal patterns in time. For example if a request fails and keeps retrying until failure, it could potentially find these abnormal retrials. [HERE](https://medium.com/@pw33392/discover-unusual-patterns-in-time-series-data-with-unsupervised-anomaly-detection-and-isolation-78db408caaed) is an example that shows how IForest finds changes within a pattern, which can be implemented into our embedding vectorstore with the logs if they are preprocessed for specific purposes.\n",
    "\n",
    "#### Embedding model (mxbai-embed-large)\n",
    "One large setback at this moment is the embedding model that we use. It is trained to find a semantic meaning in natural language, while we want to find a more semantic meaning for one or multiple logs. Therefore to get a more accurate algorithm, we need to have an embedding model that is specifically trained and used for our logs. It is important that the embedding model understand the language and sentences in a log, which is different from the natural language. If we have a model that is better with logs,  we could be able to find abnormalities that isn't only different in a textual pattern.\n",
    "\n",
    "#### Preprocessing\n",
    "We haven't in this notebook made any deeper analysis of the embedded vectors for the anomaly detection. One important part of a successful IForest is the quality of the data. We assume the quality is good enough, but in a greater implementation, some preprocessing steps may be necessary. Example of steps that could help the anomaly detection are normalize the data or reduce the amount of data to a specific UUID."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
